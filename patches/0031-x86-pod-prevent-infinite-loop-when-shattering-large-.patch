From b693c06d96adf99aaa34b0e8b53df6f03225dcae Mon Sep 17 00:00:00 2001
From: Julien Grall <julien.grall@linaro.org>
Date: Mon, 11 Dec 2017 13:33:54 +0100
Subject: [PATCH 31/33] x86/pod: prevent infinite loop when shattering large
 pages

When populating pages, the PoD may need to split large ones using
p2m_set_entry and request the caller to retry (see ept_get_entry for
instance).

p2m_set_entry may fail to shatter if it is not possible to allocate
memory for the new page table. However, the error is not propagated
resulting to the callers to retry infinitely the PoD.

Prevent the infinite loop by return false when it is not possible to
shatter the large mapping.

This is XSA-246.

Signed-off-by: Julien Grall <julien.grall@linaro.org>
Signed-off-by: Jan Beulich <jbeulich@suse.com>
Reviewed-by: George Dunlap <george.dunlap@citrix.com>

Adapted to Xen 4.1 by Peter Dreuw <peter.dreuw@credativ.de>
Signed-off-by: Peter Dreuw <peter.dreuw@credativ.de>
---
 xen/arch/x86/mm/p2m.c      | 32 ++++++++++++++++++++------------
 xen/include/asm-x86/page.h |  4 ++++
 2 files changed, 24 insertions(+), 12 deletions(-)

diff --git a/xen/arch/x86/mm/p2m.c b/xen/arch/x86/mm/p2m.c
index d76438131..5117d7347 100644
--- a/xen/arch/x86/mm/p2m.c
+++ b/xen/arch/x86/mm/p2m.c
@@ -1366,7 +1366,7 @@ p2m_pod_demand_populate(struct p2m_domain *p2m, unsigned long gfn,
     struct page_info *p = NULL; /* Compiler warnings */
     unsigned long gfn_aligned;
     mfn_t mfn;
-    int i;
+    int i, ret;
 
     ASSERT(p2m_locked_by_me(p2m));
 
@@ -1386,10 +1386,10 @@ p2m_pod_demand_populate(struct p2m_domain *p2m, unsigned long gfn,
          * set_p2m_entry() should automatically shatter the 1GB page into 
          * 512 2MB pages. The rest of 511 calls are unnecessary.
          */
-        set_p2m_entry(p2m, gfn_aligned, _mfn(POPULATE_ON_DEMAND_MFN), 9,
-                      p2m_populate_on_demand, p2m->default_access);
+        ret = set_p2m_entry(p2m, gfn_aligned, _mfn(POPULATE_ON_DEMAND_MFN), 9,
+                            p2m_populate_on_demand, p2m->default_access);
         audit_p2m(p2m, 1);
-        return 0;
+        return ret;
     }
 
     pod_eager_reclaim(p2m);
@@ -1422,7 +1422,12 @@ p2m_pod_demand_populate(struct p2m_domain *p2m, unsigned long gfn,
 
     gfn_aligned = (gfn >> order) << order;
 
-    set_p2m_entry(p2m, gfn_aligned, mfn, order, p2m_ram_rw, p2m->default_access);
+    if ( set_p2m_entry(p2m, gfn_aligned, mfn, order, p2m_ram_rw,
+                       p2m->default_access) )
+    {
+        p2m_pod_cache_add(p2m, p, order);
+        goto out_fail;
+    }
 
     for( i = 0; i < (1UL << order); i++ )
     {
@@ -1466,9 +1471,10 @@ remap_and_retry:
 
     /* Remap this 2-meg region in singleton chunks */
     gfn_aligned = (gfn>>order)<<order;
-    for(i=0; i<(1<<order); i++)
-        set_p2m_entry(p2m, gfn_aligned+i, _mfn(POPULATE_ON_DEMAND_MFN), 0,
-                      p2m_populate_on_demand, p2m->default_access);
+    if ( set_p2m_entry(p2m, gfn_aligned, _mfn(INVALID_MFN), PAGE_ORDER_4K,
+                       p2m_populate_on_demand, p2m->default_access) )
+         return -1;
+
     if ( tb_init_done )
     {
         struct {
@@ -1688,7 +1694,7 @@ p2m_gfn_to_mfn(struct p2m_domain *p2m, unsigned long gfn, p2m_type_t *t, p2m_acc
     l1_pgentry_t *l1e;
 
     ASSERT(paging_mode_translate(p2m->domain));
-
+ 
     /* XXX This is for compatibility with the old model, where anything not 
      * XXX marked as RAM was considered to be emulated MMIO space.
      * XXX Once we start explicitly registering MMIO regions in the p2m 
@@ -2056,7 +2062,7 @@ int set_p2m_entry(struct p2m_domain *p2m, unsigned long gfn, mfn_t mfn,
     struct domain *d = p2m->domain;
     unsigned long todo = 1ul << page_order;
     unsigned int order;
-    int rc = 1;
+    int set_rc, rc = 1;
 
     while ( todo )
     {
@@ -2068,8 +2074,10 @@ int set_p2m_entry(struct p2m_domain *p2m, unsigned long gfn, mfn_t mfn,
         else
             order = 0;
 
-        if ( !p2m->set_entry(p2m, gfn, mfn, order, p2mt, p2ma) )
-            rc = 0;
+        set_rc = p2m->set_entry(p2m, gfn, mfn, order, p2mt, p2ma);
+        if ( set_rc )
+            rc = set_rc;
+        
         gfn += 1ul << order;
         if ( mfn_x(mfn) != INVALID_MFN )
             mfn = _mfn(mfn_x(mfn) + (1ul << order));
diff --git a/xen/include/asm-x86/page.h b/xen/include/asm-x86/page.h
index fdbe93c73..8bb3d90b4 100644
--- a/xen/include/asm-x86/page.h
+++ b/xen/include/asm-x86/page.h
@@ -13,6 +13,10 @@
 #define PAGE_MASK           (~(PAGE_SIZE-1))
 #define PAGE_FLAG_MASK      (~0)
 
+#define PAGE_ORDER_4K       0
+#define PAGE_ORDER_2M       9
+#define PAGE_ORDER_1G       18
+
 #ifndef __ASSEMBLY__
 # include <asm/types.h>
 # include <xen/lib.h>
-- 
2.15.1

